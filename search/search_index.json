{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Open the AI Context Window with UltraRepo","text":""},{"location":"#got-massive-code-or-document-archives","title":"Got Massive Code or Document Archives?","text":"<p>UltraRepo RAG builds advanced Knowledge Graphs from your source code, documents, or unstructured data \u2014  giving AI Agents the context they need to rapidly learn, infer and to reason intelligently. Open the AI Context Window to infinity and beyond! </p> <p>No need to upload gigabytes of code into LLM memory \u2014 let Graph RAG do the heavy lifting.</p>"},{"location":"#why-choose-ultrarepo-rag","title":"Why Choose UltraRepo RAG?","text":"<ul> <li>\u2728 AI Agent Discoverable Code, Content, Data provides accuracy with no limits</li> <li>\ud83d\udce6 Graph RAG transforms code, docs, and data into an intelligent knowledge base  </li> <li>\ud83e\udde0 Optimized for hybrid symbolic + semantic search with <code>Neo4j</code> and <code>Qdrant</code> </li> <li>\ud83d\udd10 Local, secure, and offline-ready for privacy-sensitive applications</li> </ul> <p>For more information, visit UltraRepo.com</p>"},{"location":"index-old/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"index-old/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"index-old/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"Getting%20Started/1-Intro/getting-started/","title":"Getting Started","text":"<p>Welcome to the UltraRepo Graph RAG project! Follow these steps to get up and running:</p>"},{"location":"Getting%20Started/1-Intro/getting-started/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/UltraRepo/graph-rag.git\ncd graph-rag\n</code></pre>"},{"location":"Getting%20Started/1-Intro/getting-started/#2-install-dependencies","title":"2. Install Dependencies","text":"<p>Make sure you have Python and pip installed. Then run:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"Getting%20Started/1-Intro/getting-started/#3-start-the-server","title":"3. Start the Server","text":"<pre><code>uvicorn src.main:app --reload\n</code></pre> <p>You're ready to go!</p>"},{"location":"Getting%20Started/1-Intro/project_docs/","title":"LLM Knowledge Graph Builder","text":""},{"location":"Getting%20Started/1-Intro/project_docs/#introduction","title":"Introduction","text":"<p>This document provides comprehensive documentation for the Neo4j llm-graph-builder Project, a Python web application built with the FastAPI framework. It covers various aspects of the project, including its features, architecture, usage, development, deployment, limitations and known issues.</p>"},{"location":"Getting%20Started/1-Intro/project_docs/#features","title":"Features","text":"<ul> <li>Upload unstructured data from multiple sources to generate structuted Neo4j knowledge graph.</li> <li>Extraction of nodes and relations from multiple LLMs(OpenAI GPT-3.5, OpenAI GPT-4, Gemini 1.0-Pro and Diffbot).</li> <li>View complete graph or only a particular element of graph(ex: Only chunks, only entities, document and entities, etc.) </li> <li>Generate embedding of chunks created from unstructured content.</li> <li>Generate k-nearest neighbors graph for similar chunks.</li> <li>Chat with graph data using chat bot.</li> </ul>"},{"location":"Getting%20Started/1-Intro/project_docs/#local-setup-and-execution","title":"Local Setup and Execution","text":"<p>Run Docker Compose to build and start all components: <pre><code>docker-compose up --build\n</code></pre></p> <p>Alternatively, run specific directories separately:</p> <p>For frontend <pre><code>cd frontend\nyarn\nyarn run dev\n</code></pre></p> <p>For backend <pre><code>cd backend\npython -m venv envName\nsource envName/bin/activate \npip install -r requirements.txt\nuvicorn score:app --reload\n</code></pre></p> <p>Set up environment variables: <pre><code>OPENAI_API_KEY = \"\"\nDIFFBOT_API_KEY = \"\"\nNEO4J_URI = \"\"\nNEO4J_USERNAME = \"\"\nNEO4J_PASSWORD = \"\"\nNEO4J_DATABASE = \"\"\nAWS_ACCESS_KEY_ID =  \"\"\nAWS_SECRET_ACCESS_KEY = \"\"\nEMBEDDING_MODEL = \"\"\nIS_EMBEDDING = \"TRUE\"\nKNN_MIN_SCORE = \"\"\nLANGCHAIN_API_KEY = \"\"\nLANGCHAIN_PROJECT = \"\"\nLANGCHAIN_TRACING_V2 = \"\"\nLANGCHAIN_ENDPOINT = \"\"\nNUMBER_OF_CHUNKS_TO_COMBINE = \"\"\n</code></pre></p>"},{"location":"Getting%20Started/1-Intro/project_docs/#architecture","title":"Architecture","text":""},{"location":"Getting%20Started/1-Intro/project_docs/#development","title":"Development","text":""},{"location":"Getting%20Started/1-Intro/project_docs/#backend","title":"Backend","text":"<p>Backend Documentation</p>"},{"location":"Getting%20Started/1-Intro/project_docs/#frontend","title":"Frontend","text":"<p>Frontend Documentation</p>"},{"location":"Getting%20Started/1-Intro/project_docs/#deployment-and-monitoring","title":"Deployment and Monitoring","text":"<ul> <li>The application is deployed on Google Cloud Platform.</li> </ul> <p>To deploy frontend: <pre><code>gcloud run deploy \nsource location current directory &gt; Frontend\nregion : 32 [us-central 1]\nAllow unauthenticated request : Yes\n</code></pre></p> <p>To deploy backend: <pre><code>gcloud run deploy --set-env-vars \"OPENAI_API_KEY = \" --set-env-vars \"DIFFBOT_API_KEY = \" --set-env-vars \"NEO4J_URI = \" --set-env-vars \"NEO4J_PASSWORD = \" --set-env-vars \"NEO4J_USERNAME = \"\nsource location current directory &gt; Backend\nregion : 32 [us-central 1]\nAllow unauthenticated request : Yes\n</code></pre></p> <ul> <li>Langserve is used with FAST API to deploy Langchain runnables and chains as a REST API.</li> <li>Langsmith is used to monitor and evaluate the application</li> </ul> <p>Development url: [TBD]</p> <p>Production url: [TBD]</p>"},{"location":"Getting%20Started/1-Intro/project_docs/#appendix","title":"Appendix","text":""},{"location":"Getting%20Started/1-Intro/project_docs/#limitations","title":"Limitations","text":"<ul> <li>Only pdf file uploaded from device or uploaded from s3 bucket or gcs bucket can be processed.</li> <li>GCS buckets present under 1051503595507@cloudbuild.gserviceaccount.com service account can only be accessed.</li> <li>Only 1<sup>st</sup> page of Wikipedia content is processed to generate graphDocument.</li> </ul>"},{"location":"Getting%20Started/1-Intro/project_docs/#known-issues","title":"Known issues","text":"<ul> <li>InactiveRpcError error with Gemini 1.0 Pro -  grpc_status:13, grpc_message:\"Internal error encountered.\"</li> <li>ResourceExhausted error with Gemini 1.5 Pro - 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-pro</li> <li>Gemini response validation errors even after making safety_settings parameters to BLOCK_NONE. </li> </ul>"},{"location":"Getting%20Started/2-Config/","title":"UltraRepo Graph RAG","text":"<p>Online Docs:  https://UltraRepo.github.io/graph-rag/</p>"},{"location":"Getting%20Started/2-Config/#problem-overview","title":"\ud83d\udd0d Problem Overview","text":"<p>AI Agents and systems have a limited context window (typically &lt;2M tokens), which prevents them from ingesting and reasoning over large codebases, datasets, and document archives in one pass. This context window limitation restricts the AI\u2019s ability to provide accurate answers across an entire repository, whether its code, docs or data.</p>"},{"location":"Getting%20Started/2-Config/#solution-overview","title":"\ud83d\udca1 Solution Overview","text":"<p>Unlike basic RAG solutions that lack a knowledge graph to 'map' content for discoverability by AI, UltraRepo Graph RAG combines the power of RAG with the intelligence, accuracy and AI discoverability offered by a knowledge graph (KG). Lightweight metadata indexing is provided by the KG via the Neo4j graph database, and vector data is stored independently in a scalable, searchable vector DB such as Qdrant, Weaviate, Pinecone or others. Private AI is offered through the a self-hosted LLM option with Ollama; online LLMs are also available for use in retrieval and processing. Save time when working with LLMs and get better accuracy.  Instead of requiring full content uploads during chat sessions with limited content windows, UltraRepo Graph Rag helps you by scanning repo files, then processing and embedding the repo source files into your vector DB, and then links these source files using the UltraRepo Graph, which is a'map' to your content items.  Content such as blobs and files can be stored separately in a separate database, in a git worktree, or as simple files/folders in a filesystem.</p> <p>Our objective is to provide an open source project that has offers smart AI-powered understanding and retrieval of content from massive software and document repositories, regardless of context window size, across multiple LLMs, both private and public, to both AI Agents and human operated AI Chat clients.  With UltraRepo Graph Rag,  improving LLM reasoning, traceability, and extensibility.</p> <p>File Type Support:  Initially support is provided for text files and PDF files.  </p> <p>Repo File, DB Scanning:  This project has basic file uploads of repo files to the Graph Rag server. If you need repo file and DB scanning with KG and smart classification and embedding, checkout UltraRepo's commercial version (coming soon!) at UltraRepo.com.</p>"},{"location":"Getting%20Started/2-Config/#project-overview","title":"\ud83d\ude80 Project Overview","text":"<p>UltraRepo Graph RAG is based on the open source Neo4j LLM Graph Builder, with added support for:</p> <ul> <li>Advanced Graph RAG processing</li> <li>Smart Code/Data indexing for AI Agents</li> <li>Qdrant vector search integration</li> <li>Rich FastAPI backends</li> <li>Full MkDocs Material documentation support</li> </ul> <p>This project forms the backend engine of a \u201csmart repository\u201d designed for use with local, hybrid, or cloud-based AI Agents.</p>"},{"location":"Getting%20Started/2-Config/#installation-mkdocs-material-theme","title":"\ud83d\udce6 Installation: MkDocs &amp; Material Theme","text":""},{"location":"Getting%20Started/2-Config/#1-setup-virtual-environment","title":"1\ufe0f\u20e3 Setup Virtual Environment","text":"<pre><code>python -m venv .venv\nsource .venv/bin/activate  # Windows: .venv\\Scripts\\activate\n</code></pre>"},{"location":"Getting%20Started/2-Config/#2-install-dependencies","title":"2\ufe0f\u20e3 Install Dependencies","text":"<pre><code>pip install mkdocs-material\npip install mkdocs-git-revision-date-localized-plugin\npip install mkdocs-minify-plugin\n</code></pre>"},{"location":"Getting%20Started/2-Config/#3-sample-mkdocsyml-configuration","title":"3\ufe0f\u20e3 Sample <code>mkdocs.yml</code> Configuration","text":"<pre><code>site_name: UltraRepo-Graph-RAG Documentation\ntheme:\n  name: material\n  features:\n    - navigation.tabs\n    - search.highlight\n    - content.code.copy\nplugins:\n  - search\n  - git-revision-date-localized\n  - minify\nmarkdown_extensions:\n  - admonition\n  - codehilite\n  - footnotes\n  - pymdownx.superfences\n  - toc:\n      permalink: true\n</code></pre> <p>Recommended MkDocs Material theme plugins to install <pre><code>pip install \\\n  mkdocs-material \\\n  mkdocs-minify-plugin \\\n  mkdocs-git-revision-date-localized-plugin \\\n  mkdocs-glightbox \\\n  pymdown-extensions\n  ```\n\n\n### 4\ufe0f\u20e3 Serve Docs\n\n```bash\nmkdocs serve\n# Open http://127.0.0.1:8000\n</code></pre></p>"},{"location":"Getting%20Started/2-Config/#repo-structure-derived-from-llm-graph-builder","title":"\ud83d\udcc1 Repo Structure (derived from llm-graph-builder)","text":"<pre><code>[ultrarepo-graph-rag/](https://github.com/UltraRepo/graph-rag/)\n- `ultrarepo-graph-rag/`: Root of the UltraRepo Graph-RAG project\n- `\u251c\u2500\u2500 LICENSE`: License file (MIT or similar)\n- `\u251c\u2500\u2500 README.md`: Main project documentation and overview\n- `\u251c\u2500\u2500 data/`: Sample files and PDF input data for ingestion\n- `\u251c\u2500\u2500 docs/`: Markdown documentation for MkDocs site\n- `\u251c\u2500\u2500 mkdocs.yml`: Configuration file for MkDocs Material documentation site\n- `\u251c\u2500\u2500 src/`: Primary application source code directory\n- `\u2502   \u251c\u2500\u2500 backend/`: FastAPI backend server logic\n- `\u2502   \u2502   \u251c\u2500\u2500 api/`: API initialization and launch code\n- `\u2502   \u2502   \u251c\u2500\u2500 utils/`: Utility functions and helper modules\n- `\u2502   \u251c\u2500\u2500 constants/`: Global constants used across services\n- `\u2502   \u251c\u2500\u2500 embeddings/`: Embedding model classes (OpenAI, SentenceTransformers, etc.)\n- `\u2502   \u251c\u2500\u2500 graph_generation/`: Graph schema and graph extraction logic\n- `\u2502   \u2502   \u251c\u2500\u2500 graph_generation.py`: Transforms input text into nodes and relationships\n- `\u2502   \u2502   \u251c\u2500\u2500 make_relationships.py`: Creates/refines relationships between graph nodes\n- `\u2502   \u251c\u2500\u2500 models/`: Pydantic models for FastAPI request/response validation\n- `\u2502   \u251c\u2500\u2500 post_processing/`: KNN graph linking and full-text indexing\n- `\u2502   \u2502   \u251c\u2500\u2500 post_processing.py`: Similarity vector linking, KNN construction\n- `\u2502   \u251c\u2500\u2500 retrievers/`: Vector and hybrid retrieval logic (Neo4j + Qdrant)\n- `\u2502   \u251c\u2500\u2500 routers/`: FastAPI endpoints and route logic\n- `\u2502   \u2502   \u251c\u2500\u2500 chat.py`: Handles chat endpoints and response orchestration\n- `\u2502   \u2502   \u251c\u2500\u2500 connection.py`: Neo4j DB connection handlers\n- `\u2502   \u2502   \u251c\u2500\u2500 documents.py`: APIs to handle document scanning and ingestion\n- `\u2502   \u2502   \u251c\u2500\u2500 upload.py`: Chunked file upload endpoint logic\n- `\u2502   \u251c\u2500\u2500 schemas/`: Custom schema templates for entities and relationships\n- `\u2502   \u251c\u2500\u2500 services/`: Business logic for API endpoints\n- `\u2502   \u251c\u2500\u2500 types/`: Typed definitions, enums, data classes\n- `\u251c\u2500\u2500 tests/`: Unit and integration test cases\n</code></pre> <ul> <li><code>\ud83d\udcdd Key Backend Modules</code>: </li> <li>\u2022 graphDB_dataAccess.py: Located in <code>src/backend/api/</code> - Handles raw database operations and queries</li> <li>\u2022 graph_generation.py: Located in <code>src/graph_generation/</code> - Generates GraphDocument representations from LLM</li> <li>\u2022 make_relationships.py: Located in <code>src/graph_generation/</code> - Refines node/edge creation and schema matching</li> <li>\u2022 post_processing.py: Located in <code>src/post_processing/</code> - Handles KNN vector linkages and post graph cleanup</li> </ul>"},{"location":"Getting%20Started/2-Config/#documentation-and-extensibility","title":"\ud83d\udcda Documentation and Extensibility","text":"<p>UltraRepo Graph RAG is fully compatible with MkDocs Material, making it easy to extend with internal documentation, tagging, search, and AI-ready summaries.</p> <p>For more details on how to use GraphRAG with Neo4j, Qdrant, and LangChain, visit: \ud83d\udc49 https://neo4j.com/docs/neo4j-graphrag-python/current/</p>"},{"location":"Getting%20Started/2-Config/#knowledge-graph-builder-app","title":"Knowledge Graph Builder App","text":"<p>The following details provide background and installation info for the knowledge graph builder app.  This info is provided courtesy of Neo4j's Graph RAG project team.  The Neo4j Graph Rag project is an open source, MIT project that provides knowledge graphs from unstructured data.</p>"},{"location":"Getting%20Started/2-Config/#llm-graph-builder","title":"LLM Graph Builder","text":""},{"location":"Getting%20Started/2-Config/#overview","title":"Overview","text":"<p>This application is designed to turn Unstructured data (pdfs,docs,txt,youtube video,web pages,etc.) into a knowledge graph stored in Neo4j. It utilizes the power of Large language models (OpenAI,Gemini,etc.) to extract nodes, relationships and their properties from the text and create a structured knowledge graph using Langchain framework. </p> <p>Upload your files from local machine, GCS or S3 bucket or from web sources, choose your LLM model and generate knowledge graph.</p>"},{"location":"Getting%20Started/2-Config/#key-features","title":"Key Features","text":"<ul> <li>Knowledge Graph Creation: Transform unstructured data into structured knowledge graphs using LLMs.</li> <li>Providing Schema: Provide your own custom schema or use existing schema in settings to generate graph.</li> <li>View Graph: View graph for a particular source or multiple sources at a time in Bloom.</li> <li>Chat with Data: Interact with your data in a Neo4j database through conversational queries, also retrieve metadata about the source of response to your queries.For a dedicated chat interface, access the standalone chat application at: Chat-Only. This link provides a focused chat experience for querying your data.</li> </ul>"},{"location":"Getting%20Started/2-Config/#getting-started","title":"Getting started","text":"<p> You will need to have a Neo4j Database 5.23 or later with APOC installed to use this Knowledge Graph Builder. You can use any Neo4j Aura database (including the free database) If you are using Neo4j Desktop, you will not be able to use the docker-compose but will have to follow the separate deployment of backend and frontend section. </p>"},{"location":"Getting%20Started/2-Config/#deployment","title":"Deployment","text":""},{"location":"Getting%20Started/2-Config/#local-deployment","title":"Local deployment","text":""},{"location":"Getting%20Started/2-Config/#running-through-docker-compose","title":"Running through docker-compose","text":"<p>By default only OpenAI and Diffbot are enabled since Gemini requires extra GCP configurations. According to enviornment we are configuring the models which is indicated by VITE_LLM_MODELS_PROD variable we can configure model based on our need.</p> <p>EX: <pre><code>VITE_LLM_MODELS_PROD=\"openai_gpt_4o,openai_gpt_4o_mini,diffbot,gemini_1.5_flash\"\n</code></pre></p>"},{"location":"Getting%20Started/2-Config/#additional-configs","title":"Additional configs","text":"<p>By default, the input sources will be: Local files, Youtube, Wikipedia ,AWS S3 and Webpages. As this default config is applied: <pre><code>VITE_REACT_APP_SOURCES=\"local,youtube,wiki,s3,web\"\n</code></pre></p> <p>If however you want the Google GCS integration, add <code>gcs</code> and your Google client ID: <pre><code>VITE_REACT_APP_SOURCES=\"local,youtube,wiki,s3,gcs,web\"\nVITE_GOOGLE_CLIENT_ID=\"xxxx\"\n</code></pre></p> <p>You can of course combine all (local, youtube, wikipedia, s3 and gcs) or remove any you don't want/need.</p>"},{"location":"Getting%20Started/2-Config/#chat-modes","title":"Chat Modes","text":"<p>By default,all of the chat modes will be available: vector, graph_vector, graph, fulltext, graph_vector_fulltext , entity_vector and global_vector.</p> <p>If none of the mode is mentioned in the chat modes variable all modes will be available: <pre><code>VITE_CHAT_MODES=\"\"\n</code></pre></p> <p>If however you want to specify the only vector mode or only graph mode you can do that by specifying the mode in the env: <pre><code>VITE_CHAT_MODES=\"vector,graph\"\nVITE_CHAT_MODES=\"vector,graph\"\n</code></pre></p>"},{"location":"Getting%20Started/2-Config/#running-backend-and-frontend-separately-dev-environment","title":"Running Backend and Frontend separately (dev environment)","text":"<p>Alternatively, you can run the backend and frontend separately:</p> <ul> <li>For the frontend:</li> <li>Create the frontend/.env file by copy/pasting the frontend/example.env.</li> <li> <p>Change values as needed 3.     <pre><code>cd frontend\nyarn\nyarn run dev\n</code></pre></p> </li> <li> <p>For the backend:</p> </li> <li>Create the backend/.env file by copy/pasting the backend/example.env. To streamline the initial setup and testing of the application, you can preconfigure user credentials directly within the backend .env file. This bypasses the login dialog and allows you to immediately connect with a predefined user.</li> <li>NEO4J_URI:</li> <li>NEO4J_USERNAME:</li> <li>NEO4J_PASSWORD:</li> <li>NEO4J_DATABASE:</li> <li>Change values as needed 4.     <pre><code>cd backend\npython -m venv envName\nsource envName/bin/activate \npip install -r requirements.txt\nuvicorn score:app --reload\n</code></pre></li> </ul>"},{"location":"Getting%20Started/2-Config/#deploy-in-cloud","title":"Deploy in Cloud","text":"<p>To deploy the app and packages on Google Cloud Platform, run the following command on google cloud run: <pre><code># Frontend deploy \ngcloud run deploy dev-frontend \nsource location current directory &gt; Frontend\nregion : 32 [us-central 1]\nAllow unauthenticated request : Yes\n</code></pre> <pre><code># Backend deploy \ngcloud run deploy --set-env-vars \"OPENAI_API_KEY = \" --set-env-vars \"DIFFBOT_API_KEY = \" --set-env-vars \"NEO4J_URI = \" --set-env-vars \"NEO4J_PASSWORD = \" --set-env-vars \"NEO4J_USERNAME = \"\nsource location current directory &gt; Backend\nregion : 32 [us-central 1]\nAllow unauthenticated request : Yes\n</code></pre></p>"},{"location":"Getting%20Started/2-Config/#env","title":"ENV","text":"Env Variable Name Mandatory/Optional Default Value Description BACKEND ENV OPENAI_API_KEY Mandatory An OpenAPI Key is required to use open LLM model to authenticate andn track requests DIFFBOT_API_KEY Mandatory API key is required to use Diffbot's NLP service to extraction entities and relatioship from unstructured data BUCKET Mandatory bucket name to store uploaded file on GCS NEO4J_USER_AGENT Optional llm-graph-builder Name of the user agent to track neo4j database activity ENABLE_USER_AGENT Optional true Boolean value to enable/disable neo4j user agent DUPLICATE_TEXT_DISTANCE Mandatory 5 This value used to find distance for all node pairs in the graph and calculated based on node properties DUPLICATE_SCORE_VALUE Mandatory 0.97 Node score value to match duplicate node EFFECTIVE_SEARCH_RATIO Mandatory 1 GRAPH_CLEANUP_MODEL Optional 0.97 Model name to clean-up graph in post processing MAX_TOKEN_CHUNK_SIZE Optional 10000 Maximum token size to process file content YOUTUBE_TRANSCRIPT_PROXY Optional Proxy key to process youtube video for getting transcript EMBEDDING_MODEL Optional all-MiniLM-L6-v2 Model for generating the text embedding (all-MiniLM-L6-v2 , openai , vertexai) IS_EMBEDDING Optional true Flag to enable text embedding KNN_MIN_SCORE Optional 0.94 Minimum score for KNN algorithm GEMINI_ENABLED Optional False Flag to enable Gemini GCP_LOG_METRICS_ENABLED Optional False Flag to enable Google Cloud logs NUMBER_OF_CHUNKS_TO_COMBINE Optional 5 Number of chunks to combine when processing embeddings UPDATE_GRAPH_CHUNKS_PROCESSED Optional 20 Number of chunks processed before updating progress NEO4J_URI Optional neo4j://database:7687 URI for Neo4j database NEO4J_USERNAME Optional neo4j Username for Neo4j database NEO4J_PASSWORD Optional password Password for Neo4j database LANGCHAIN_API_KEY Optional API key for Langchain LANGCHAIN_PROJECT Optional Project for Langchain LANGCHAIN_TRACING_V2 Optional true Flag to enable Langchain tracing GCS_FILE_CACHE Optional False If set to True, will save the files to process into GCS. If set to False, will save the files locally LANGCHAIN_ENDPOINT Optional https://api.smith.langchain.com Endpoint for Langchain API ENTITY_EMBEDDING Optional False If set to True, It will add embeddings for each entity in database LLM_MODEL_CONFIG_ollama_ Optional Set ollama config as - model_name,model_local_url for local deployments RAGAS_EMBEDDING_MODEL Optional openai embedding model used by ragas evaluation framework FRONTEND ENV VITE_BACKEND_API_URL Optional http://localhost:8000 URL for backend API VITE_BLOOM_URL Optional https://workspace-preview.neo4j.io/workspace/explore?connectURL={CONNECT_URL}&amp;search=Show+me+a+graph&amp;featureGenAISuggestions=true&amp;featureGenAISuggestionsInternal=true URL for Bloom visualization VITE_REACT_APP_SOURCES Mandatory local,youtube,wiki,s3 List of input sources that will be available VITE_CHAT_MODES Mandatory vector,graph+vector,graph,hybrid Chat modes available for Q&amp;A VITE_ENV Mandatory DEV or PROD Environment variable for the app VITE_TIME_PER_PAGE Optional 50 Time per page for processing VITE_CHUNK_SIZE Optional 5242880 Size of each chunk of file for upload VITE_GOOGLE_CLIENT_ID Optional Client ID for Google authentication VITE_LLM_MODELS_PROD Optional openai_gpt_4o,openai_gpt_4o_mini,diffbot,gemini_1.5_flash To Distinguish models based on the Enviornment PROD or DEV VITE_LLM_MODELS Optional 'diffbot,openai_gpt_3.5,openai_gpt_4o,openai_gpt_4o_mini,gemini_1.5_pro,gemini_1.5_flash,azure_ai_gpt_35,azure_ai_gpt_4o,ollama_llama3,groq_llama3_70b,anthropic_claude_3_5_sonnet' Supported Models For the application VITE_AUTH0_CLIENT_ID Mandatory if you are enabling Authentication otherwise it is optional Okta Oauth Client ID for authentication VITE_AUTH0_DOMAIN Mandatory if you are enabling Authentication otherwise it is optional Okta Oauth Cliend Domain VITE_SKIP_AUTH Optional true Flag to skip the authentication VITE_CHUNK_OVERLAP Optional 20 variable to configure chunk overlap VITE_TOKENS_PER_CHUNK Optional 100 variable to configure tokens count per chunk.This gives flexibility for users who may require different chunk sizes for various tokenization tasks, especially when working with large datasets or specific language models. VITE_CHUNK_TO_COMBINE Optional 1 variable to configure number of chunks to combine for parllel processing."},{"location":"Getting%20Started/2-Config/#llms-supported","title":"LLMs Supported","text":"<ol> <li>OpenAI</li> <li>Gemini</li> <li>Diffbot</li> <li>Azure OpenAI(dev deployed version)</li> <li>Anthropic(dev deployed version)</li> <li>Fireworks(dev deployed version)</li> <li>Groq(dev deployed version)</li> <li>Amazon Bedrock(dev deployed version)</li> <li>Ollama(dev deployed version)</li> <li>Deepseek(dev deployed version)</li> <li>Other OpenAI compabtile baseurl models(dev deployed version)</li> </ol>"},{"location":"Getting%20Started/2-Config/#for-local-llms-ollama","title":"For local llms (Ollama)","text":"<ol> <li>Pull the docker imgage of ollama <pre><code>docker pull ollama/ollama\n</code></pre></li> <li>Run the ollama docker image <pre><code>docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n</code></pre></li> <li>Pull specific ollama model. <pre><code>ollama pull llama3\n</code></pre></li> <li>Execute any llm model ex\ud83e\udd993 <pre><code>docker exec -it ollama ollama run llama3\n</code></pre></li> <li>Configure  env variable in docker compose. <pre><code>LLM_MODEL_CONFIG_ollama_&lt;model_name&gt;\n#example\nLLM_MODEL_CONFIG_ollama_llama3=${LLM_MODEL_CONFIG_ollama_llama3-llama3,\nhttp://host.docker.internal:11434}\n</code></pre></li> <li>Configure the backend API url <pre><code>VITE_BACKEND_API_URL=${VITE_BACKEND_API_URL-backendurl}\n</code></pre></li> <li>Open the application in browser and select the ollama model for the extraction.</li> <li>Enjoy Graph Building.</li> </ol>"},{"location":"Getting%20Started/2-Config/#usage","title":"Usage","text":"<ol> <li>Connect to Neo4j Aura Instance which can be both AURA DS or AURA DB by passing URI and password through Backend env, fill using login dialog or drag and drop the Neo4j credentials file.</li> <li>To differntiate we have added different icons. For AURA DB we have a database icon and for AURA DS we have scientific molecule icon right under Neo4j Connection details label.</li> <li>Choose your source from a list of Unstructured sources to create graph.</li> <li>Change the LLM (if required) from drop down, which will be used to generate graph.</li> <li>Optionally, define schema(nodes and relationship labels) in entity graph extraction settings.</li> <li>Either select multiple files to 'Generate Graph' or all the files in 'New' status will be processed for graph creation.</li> <li>Have a look at the graph for individual files using 'View' in grid or select one or more files and 'Preview Graph'</li> <li>Ask questions related to the processed/completed sources to chat-bot, Also get detailed information about your answers generated by LLM.</li> </ol>"},{"location":"Getting%20Started/2-Config/#links","title":"Links","text":"<p>LLM Knowledge Graph Builder Application</p> <p>Neo4j Workspace</p>"},{"location":"Getting%20Started/2-Config/#reference","title":"Reference","text":"<p>Demo of application</p>"},{"location":"Getting%20Started/2-Config/#contact","title":"Contact","text":"<p>For any inquiries or support, feel free to raise Github Issue</p>"},{"location":"Getting%20Started/2-Config/#license","title":"\u2696\ufe0f License","text":"<p>Apache-2.0 license.  Feel free to use and extend.</p>"},{"location":"Getting%20Started/3-Deploy/Deployment/","title":"Deployment Guide","text":"","tags":["deployment","mkdocs","github pages","docs"]},{"location":"Getting%20Started/3-Deploy/Deployment/#deployment-guide-for-ultrarepo-graph-rag","title":"Deployment Guide for UltraRepo Graph RAG","text":"<p>This guide will help you deploy the system on a VPS or on your local machine.</p> <p>Neo4j Setup Info</p> <p>You will need to have a Neo4j Database 5.23 or later with APOC installed to use this Knowledge Graph Builder.</p> <p>You can use any Neo4j Aura database (including the free database).</p> <p>If you are using Neo4j Desktop, you will not be able to use the <code>docker-compose</code> but will have to follow the separate deployment of backend and frontend section.</p>","tags":["deployment","mkdocs","github pages","docs"]},{"location":"Getting%20Started/3-Deploy/Deployment/#deployment","title":"Deployment","text":"","tags":["deployment","mkdocs","github pages","docs"]},{"location":"Getting%20Started/3-Deploy/Deployment/#local-deployment","title":"Local deployment","text":"","tags":["deployment","mkdocs","github pages","docs"]},{"location":"Getting%20Started/3-Deploy/Deployment/#running-through-docker-compose","title":"Running through docker-compose","text":"<p>By default only OpenAI and Diffbot are enabled since Gemini requires extra GCP configurations. According to enviornment we are configuring the models which is indicated by VITE_LLM_MODELS_PROD variable we can configure model based on our need.</p> <p>EX: <pre><code>VITE_LLM_MODELS_PROD=\"openai_gpt_4o,openai_gpt_4o_mini,diffbot,gemini_1.5_flash\"\n</code></pre></p>","tags":["deployment","mkdocs","github pages","docs"]},{"location":"Getting%20Started/3-Deploy/Deployment/#additional-configs","title":"Additional configs","text":"<p>By default, the input sources will be: Local files, Youtube, Wikipedia ,AWS S3 and Webpages. As this default config is applied: <pre><code>VITE_REACT_APP_SOURCES=\"local,youtube,wiki,s3,web\"\n</code></pre></p> <p>If however you want the Google GCS integration, add <code>gcs</code> and your Google client ID: <pre><code>VITE_REACT_APP_SOURCES=\"local,youtube,wiki,s3,gcs,web\"\nVITE_GOOGLE_CLIENT_ID=\"xxxx\"\n</code></pre></p> <p>You can of course combine all (local, youtube, wikipedia, s3 and gcs) or remove any you don't want/need.</p>","tags":["deployment","mkdocs","github pages","docs"]},{"location":"Getting%20Started/3-Deploy/Deployment/#chat-modes","title":"Chat Modes","text":"<p>By default,all of the chat modes will be available: vector, graph_vector, graph, fulltext, graph_vector_fulltext , entity_vector and global_vector.</p> <p>If none of the mode is mentioned in the chat modes variable all modes will be available: <pre><code>VITE_CHAT_MODES=\"\"\n</code></pre></p> <p>If however you want to specify the only vector mode or only graph mode you can do that by specifying the mode in the env: <pre><code>VITE_CHAT_MODES=\"vector,graph\"\nVITE_CHAT_MODES=\"vector,graph\"\n</code></pre></p>","tags":["deployment","mkdocs","github pages","docs"]},{"location":"Getting%20Started/3-Deploy/Deployment/#running-backend-and-frontend-separately-dev-environment","title":"Running Backend and Frontend separately (dev environment)","text":"<p>Alternatively, you can run the backend and frontend separately:</p> <ul> <li>For the frontend:</li> <li>Create the frontend/.env file by copy/pasting the frontend/example.env.</li> <li> <p>Change values as needed 3.     <pre><code>cd frontend\nyarn\nyarn run dev\n</code></pre></p> </li> <li> <p>For the backend:</p> </li> <li>Create the backend/.env file by copy/pasting the backend/example.env. To streamline the initial setup and testing of the application, you can preconfigure user credentials directly within the backend .env file. This bypasses the login dialog and allows you to immediately connect with a predefined user.</li> <li>NEO4J_URI:</li> <li>NEO4J_USERNAME:</li> <li>NEO4J_PASSWORD:</li> <li>NEO4J_DATABASE:</li> <li>Change values as needed 4.     <pre><code>cd backend\npython -m venv envName\nsource envName/bin/activate \npip install -r requirements.txt\nuvicorn score:app --reload\n</code></pre></li> </ul>","tags":["deployment","mkdocs","github pages","docs"]},{"location":"Getting%20Started/3-Deploy/Deployment/#deploy-in-cloud","title":"Deploy in Cloud","text":"<p>To deploy the app and packages on Google Cloud Platform, run the following command on google cloud run: <pre><code># Frontend deploy \ngcloud run deploy dev-frontend \nsource location current directory &gt; Frontend\nregion : 32 [us-central 1]\nAllow unauthenticated request : Yes\n</code></pre> <pre><code># Backend deploy \ngcloud run deploy --set-env-vars \"OPENAI_API_KEY = \" --set-env-vars \"DIFFBOT_API_KEY = \" --set-env-vars \"NEO4J_URI = \" --set-env-vars \"NEO4J_PASSWORD = \" --set-env-vars \"NEO4J_USERNAME = \"\nsource location current directory &gt; Backend\nregion : 32 [us-central 1]\nAllow unauthenticated request : Yes\n</code></pre></p>","tags":["deployment","mkdocs","github pages","docs"]},{"location":"Getting%20Started/3-Deploy/Deployment/#env","title":"ENV","text":"Env Variable Name Mandatory/Optional Default Value Description BACKEND ENV OPENAI_API_KEY Mandatory An OpenAPI Key is required to use open LLM model to authenticate andn track requests DIFFBOT_API_KEY Mandatory API key is required to use Diffbot's NLP service to extraction entities and relatioship from unstructured data BUCKET Mandatory bucket name to store uploaded file on GCS NEO4J_USER_AGENT Optional llm-graph-builder Name of the user agent to track neo4j database activity ENABLE_USER_AGENT Optional true Boolean value to enable/disable neo4j user agent DUPLICATE_TEXT_DISTANCE Mandatory 5 This value used to find distance for all node pairs in the graph and calculated based on node properties DUPLICATE_SCORE_VALUE Mandatory 0.97 Node score value to match duplicate node EFFECTIVE_SEARCH_RATIO Mandatory 1 GRAPH_CLEANUP_MODEL Optional 0.97 Model name to clean-up graph in post processing MAX_TOKEN_CHUNK_SIZE Optional 10000 Maximum token size to process file content YOUTUBE_TRANSCRIPT_PROXY Optional Proxy key to process youtube video for getting transcript EMBEDDING_MODEL Optional all-MiniLM-L6-v2 Model for generating the text embedding (all-MiniLM-L6-v2 , openai , vertexai) IS_EMBEDDING Optional true Flag to enable text embedding KNN_MIN_SCORE Optional 0.94 Minimum score for KNN algorithm GEMINI_ENABLED Optional False Flag to enable Gemini GCP_LOG_METRICS_ENABLED Optional False Flag to enable Google Cloud logs NUMBER_OF_CHUNKS_TO_COMBINE Optional 5 Number of chunks to combine when processing embeddings UPDATE_GRAPH_CHUNKS_PROCESSED Optional 20 Number of chunks processed before updating progress NEO4J_URI Optional neo4j://database:7687 URI for Neo4j database NEO4J_USERNAME Optional neo4j Username for Neo4j database NEO4J_PASSWORD Optional password Password for Neo4j database LANGCHAIN_API_KEY Optional API key for Langchain LANGCHAIN_PROJECT Optional Project for Langchain LANGCHAIN_TRACING_V2 Optional true Flag to enable Langchain tracing GCS_FILE_CACHE Optional False If set to True, will save the files to process into GCS. If set to False, will save the files locally LANGCHAIN_ENDPOINT Optional https://api.smith.langchain.com Endpoint for Langchain API ENTITY_EMBEDDING Optional False If set to True, It will add embeddings for each entity in database LLM_MODEL_CONFIG_ollama_ Optional Set ollama config as - model_name,model_local_url for local deployments RAGAS_EMBEDDING_MODEL Optional openai embedding model used by ragas evaluation framework FRONTEND ENV VITE_BACKEND_API_URL Optional http://localhost:8000 URL for backend API VITE_BLOOM_URL Optional https://workspace-preview.neo4j.io/workspace/explore?connectURL={CONNECT_URL}&amp;search=Show+me+a+graph&amp;featureGenAISuggestions=true&amp;featureGenAISuggestionsInternal=true URL for Bloom visualization VITE_REACT_APP_SOURCES Mandatory local,youtube,wiki,s3 List of input sources that will be available VITE_CHAT_MODES Mandatory vector,graph+vector,graph,hybrid Chat modes available for Q&amp;A VITE_ENV Mandatory DEV or PROD Environment variable for the app VITE_TIME_PER_PAGE Optional 50 Time per page for processing VITE_CHUNK_SIZE Optional 5242880 Size of each chunk of file for upload VITE_GOOGLE_CLIENT_ID Optional Client ID for Google authentication VITE_LLM_MODELS_PROD Optional openai_gpt_4o,openai_gpt_4o_mini,diffbot,gemini_1.5_flash To Distinguish models based on the Enviornment PROD or DEV VITE_LLM_MODELS Optional 'diffbot,openai_gpt_3.5,openai_gpt_4o,openai_gpt_4o_mini,gemini_1.5_pro,gemini_1.5_flash,azure_ai_gpt_35,azure_ai_gpt_4o,ollama_llama3,groq_llama3_70b,anthropic_claude_3_5_sonnet' Supported Models For the application VITE_AUTH0_CLIENT_ID Mandatory if you are enabling Authentication otherwise it is optional Okta Oauth Client ID for authentication VITE_AUTH0_DOMAIN Mandatory if you are enabling Authentication otherwise it is optional Okta Oauth Cliend Domain VITE_SKIP_AUTH Optional true Flag to skip the authentication VITE_CHUNK_OVERLAP Optional 20 variable to configure chunk overlap VITE_TOKENS_PER_CHUNK Optional 100 variable to configure tokens count per chunk.This gives flexibility for users who may require different chunk sizes for various tokenization tasks, especially when working with large datasets or specific language models. VITE_CHUNK_TO_COMBINE Optional 1 variable to configure number of chunks to combine for parllel processing.","tags":["deployment","mkdocs","github pages","docs"]},{"location":"Getting%20Started/3-Deploy/Deployment/#llms-supported","title":"LLMs Supported","text":"<ol> <li>OpenAI</li> <li>Gemini</li> <li>Diffbot</li> <li>Azure OpenAI(dev deployed version)</li> <li>Anthropic(dev deployed version)</li> <li>Fireworks(dev deployed version)</li> <li>Groq(dev deployed version)</li> <li>Amazon Bedrock(dev deployed version)</li> <li>Ollama(dev deployed version)</li> <li>Deepseek(dev deployed version)</li> <li>Other OpenAI compabtile baseurl models(dev deployed version)</li> </ol>","tags":["deployment","mkdocs","github pages","docs"]},{"location":"Getting%20Started/3-Deploy/Deployment/#for-local-llms-ollama","title":"For local llms (Ollama)","text":"<ol> <li>Pull the docker imgage of ollama <pre><code>docker pull ollama/ollama\n</code></pre></li> <li>Run the ollama docker image <pre><code>docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n</code></pre></li> <li>Pull specific ollama model. <pre><code>ollama pull llama3\n</code></pre></li> <li>Execute any llm model ex\ud83e\udd993 <pre><code>docker exec -it ollama ollama run llama3\n</code></pre></li> <li>Configure  env variable in docker compose. <pre><code>LLM_MODEL_CONFIG_ollama_&lt;model_name&gt;\n#example\nLLM_MODEL_CONFIG_ollama_llama3=${LLM_MODEL_CONFIG_ollama_llama3-llama3,\nhttp://host.docker.internal:11434}\n</code></pre></li> <li>Configure the backend API url <pre><code>VITE_BACKEND_API_URL=${VITE_BACKEND_API_URL-backendurl}\n</code></pre></li> <li>Open the application in browser and select the ollama model for the extraction.</li> <li>Enjoy Graph Building.</li> </ol>","tags":["deployment","mkdocs","github pages","docs"]},{"location":"Getting%20Started/3-Deploy/Deployment/#usage","title":"Usage","text":"<ol> <li>Connect to Neo4j Aura Instance which can be both AURA DS or AURA DB by passing URI and password through Backend env, fill using login dialog or drag and drop the Neo4j credentials file.</li> <li>To differntiate we have added different icons. For AURA DB we have a database icon and for AURA DS we have scientific molecule icon right under Neo4j Connection details label.</li> <li>Choose your source from a list of Unstructured sources to create graph.</li> <li>Change the LLM (if required) from drop down, which will be used to generate graph.</li> <li>Optionally, define schema(nodes and relationship labels) in entity graph extraction settings.</li> <li>Either select multiple files to 'Generate Graph' or all the files in 'New' status will be processed for graph creation.</li> <li>Have a look at the graph for individual files using 'View' in grid or select one or more files and 'Preview Graph'</li> <li>Ask questions related to the processed/completed sources to chat-bot, Also get detailed information about your answers generated by LLM.</li> </ol>","tags":["deployment","mkdocs","github pages","docs"]},{"location":"Getting%20Started/3-Deploy/Deployment/#links","title":"Links","text":"<p>LLM Knowledge Graph Builder Application</p> <p>Neo4j Workspace</p>","tags":["deployment","mkdocs","github pages","docs"]},{"location":"Getting%20Started/3-Deploy/Deployment/#reference","title":"Reference","text":"<p>Demo of application</p>","tags":["deployment","mkdocs","github pages","docs"]},{"location":"Getting%20Started/3-Deploy/Deployment/#contact","title":"Contact","text":"<p>For any inquiries or support, feel free to raise Github Issue</p>","tags":["deployment","mkdocs","github pages","docs"]},{"location":"Getting%20Started/3-Deploy/Deployment/#license","title":"\u2696\ufe0f License","text":"<p>Apache-2.0 license.  Feel free to use and extend.</p>","tags":["deployment","mkdocs","github pages","docs"]}]}